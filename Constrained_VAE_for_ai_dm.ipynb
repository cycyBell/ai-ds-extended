{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E09Bz2d13bex"
      },
      "outputs": [],
      "source": [
        "def xavier_normal_init(shape):\n",
        "  input_dim, output_dim = shape\n",
        "  std = tf.sqrt(2.)/tf.sqrt(tf.cast(input_dim + output_dim, dtype = tf.float32))\n",
        "  weight_values = tf.random.normal(shape, stddev=std)\n",
        "  return weight_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsmcpnot49Wl"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.Module):\n",
        "  def __init__(self, output_dim, weight_init_fn = xavier_normal_init, activation = tf.identity):\n",
        "    self.output_dim = output_dim\n",
        "    self.weight_init_fn = weight_init_fn\n",
        "    self.activation = activation\n",
        "    self.built = False\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if not self.built:\n",
        "      self.input_dim = x.shape[-1]\n",
        "      self.weight = tf.Variable(self.weight_init_fn((self.input_dim, self.output_dim)), name = \"weight\")\n",
        "      self.bias = tf.Variable(tf.zeros((self.output_dim, )), name = \"bias\")\n",
        "      self.built = True\n",
        "    z = tf.add(tf.matmul(x, self.weight), self.bias)\n",
        "    return self.activation(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHl0VO9q5hMW"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n",
        "  def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epsilon = epsilon\n",
        "    self.t = 1.\n",
        "    self.v_dvar, self.s_dvar = [], []\n",
        "    self.built = False\n",
        "\n",
        "  def apply_gradients(self, grads, vars):\n",
        "    if not self.built:\n",
        "      for var in vars:\n",
        "        s = tf.Variable(tf.zeros(var.shape))\n",
        "        v = tf.Variable(tf.zeros(var.shape))\n",
        "        self.v_dvar.append(v)\n",
        "        self.s_dvar.append(s)\n",
        "      self.built = True\n",
        "    for i, (grad, var) in enumerate(zip(grads, vars)):\n",
        "      self.v_dvar[i].assign(self.beta_1 * self.v_dvar[i] + (1 - self.beta_1) * grad)\n",
        "      self.s_dvar[i].assign(self.beta_2 * self.s_dvar[i] + (1 - self.beta_2) * tf.square(grad))\n",
        "      v_corrected = self.v_dvar[i] / (1 - tf.pow(self.beta_1, self.t))\n",
        "      s_corrected = self.s_dvar[i] / (1 - tf.pow(self.beta_2, self.t))\n",
        "      var.assign_sub(self.learning_rate * v_corrected / (tf.sqrt(s_corrected) + self.epsilon))\n",
        "    self.t += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BetaScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, beta, initial_beta=1.0, min_beta=0.1, decay_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.initial_beta = initial_beta\n",
        "        self.min_beta = min_beta\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        new_beta = max(self.min_beta, self.initial_beta - self.decay_rate * epoch)   #min(self.max_beta, (epoch + 1) / self.anneal_epochs * self.max_beta)\n",
        "        self.beta.assign(new_beta)\n",
        "        print(f\"Epoch {epoch+1}: Beta = {self.beta.numpy():.4f}\")\n",
        "\n",
        "# Initialize Beta\n",
        "beta = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "beta_scheduler = BetaScheduler(beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YapXM6o4859"
      },
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE_Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(Lagrange_Constrained_VAE_Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")\n",
        "    self.conv2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")  # 1 Conv Layer\n",
        "    self.pool = layers.MaxPooling2D((2, 2))\n",
        "    self.flatten = layers.GlobalMaxPooling2D()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.loc_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "    self.logvar_layer = layers.Dense(latent_dim)  # Log variance layer\n",
        "    self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "\n",
        "  def sample_latent(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch_size = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = keras.random.normal(shape=(batch_size, dim), seed = self.seed_generator)\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    mu = self.loc_layer(x)  # Mean of latent distribution\n",
        "    logvar = self.logvar_layer(x)  # Log variance\n",
        "    z = self.sample_latent([mu, logvar])\n",
        "\n",
        "    return mu, logvar, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE_Decoder2(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(Lagrange_Constrained_VAE_Decoder2, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(64 * 5 * 6, activation=\"relu\")  # Increased spatial size\n",
        "        self.reshape = layers.Reshape((5, 6, 64))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", padding=\"same\")\n",
        "        self.convT3 = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        x = self.convT3(x)  # Final output (10,12,1)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE(keras.Model):\n",
        "    def __init__(self,encoder, decoder, lambda_lagrange_init = 1.0, **kwargs):\n",
        "        super(Lagrange_Constrained_VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.lambda_lagrange = tf.Variable(lambda_lagrange_init, trainable = True, dtype = tf.float32)\n",
        "        self.total_loss = keras.metrics.Mean(name = 'total_loss')\n",
        "        self.reconstruction_loss = keras.metrics.Mean(name = 'reconstruction_loss')\n",
        "        self.kl_loss = keras.metrics.Mean(name = 'kl_loss')\n",
        "        self.constraint_loss = keras.metrics.Mean(name = 'constraints_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "      return [\n",
        "              self.total_loss,\n",
        "              self.reconstruction_loss,\n",
        "              self.kl_loss,\n",
        "              self.constraint_loss\n",
        "      ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              keras.losses.binary_crossentropy(data, reconstruction), axis=(1)\n",
        "          )\n",
        "      )\n",
        "\n",
        "\n",
        "        tf.keras.losses.MeanSquaredError(reduction = \"sum_over_batch_size\")(data, reconstruction)\n",
        "\n",
        "\n",
        "\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        constraint_loss = tf.reduce_mean(self.lambda_lagrange*tf.square(tf.reduce_sum(z_log_var, axis = 1) - 1))\n",
        "        total_loss = reconstruction_loss + kl_loss # + constraint_loss\n",
        "\n",
        "      grads = tape.gradient(total_loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "      self.total_loss.update_state(total_loss)\n",
        "      self.reconstruction_loss.update_state(reconstruction_loss)\n",
        "      self.kl_loss.update_state(kl_loss)\n",
        "      self.constraint_loss.update_state(constraint_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss.result(),\n",
        "          \"kl_loss\": self.kl_loss.result(),\n",
        "          \"constraint_loss\": self.constraint_loss.result()\n",
        "      }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW94LTBO48sI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VAE_Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(VAE_Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.dense2 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.loc_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "    self.logvar_layer = layers.Dense(latent_dim)  # Log variance layer\n",
        "    self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "\n",
        "  def sample_latent(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch_size = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = keras.random.normal(shape=(batch_size, dim), seed = self.seed_generator)\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    mu = self.loc_layer(x)  # Mean of latent distribution\n",
        "    logvar = self.logvar_layer(x)  # Log variance\n",
        "    z = self.sample_latent([mu, logvar])\n",
        "\n",
        "    return mu, logvar, z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ6LfCgD5y3b"
      },
      "outputs": [],
      "source": [
        "class VAE_Decoder(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(VAE_Decoder, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(128, activation=\"relu\")  # Increased spatial size\n",
        "        self.dense3 = layers.Dense(5*6*32, activation=\"relu\")\n",
        "        self.reshape = layers.Reshape((5, 6, 32))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XktY8rV6Igf"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.dense2 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.latent_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    z = self.latent_layer(x)  # Mean of latent distribution\n",
        "\n",
        "\n",
        "    return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JM_BhZw6O1-"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(128, activation=\"relu\")  # Increased spatial size\n",
        "        self.dense3 = layers.Dense(5*6*32, activation=\"relu\")\n",
        "        self.reshape = layers.Reshape((5, 6, 32))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkHC_jad6Tlk"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(keras.Model):\n",
        "    def __init__(self,encoder, decoder, **kwargs):\n",
        "        super(Autoencoder, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss = keras.metrics.Mean(name = 'total_loss')\n",
        "        self.reconstruction_loss = keras.metrics.Mean(name = 'reconstruction_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "      return [\n",
        "              self.total_loss,\n",
        "              self.reconstruction_loss,\n",
        "\n",
        "      ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              keras.losses.binary_crossentropy(data, reconstruction), axis=(1)\n",
        "          )\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "        total_loss = reconstruction_loss\n",
        "\n",
        "      grads = tape.gradient(total_loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "      self.total_loss.update_state(total_loss)\n",
        "      self.reconstruction_loss.update_state(reconstruction_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss.result(),\n",
        "\n",
        "      }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRRF3qlA6iy4"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "# set the dimensionality of the latent space to a plane for visualization later\n",
        "latent_dim = 2\n",
        "output_shape = (10, 12,1)\n",
        "encoder = Encoder(latent_dim)\n",
        "decoder = Decoder(output_shape)\n",
        "model = Autoencoder(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYwabjqz6kMX"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate = .0001))\n",
        "history = model.fit(train_data, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAn3c2jt8GVy"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "# set the dimensionality of the latent space to a plane for visualization later\n",
        "latent_dim = 2\n",
        "output_shape = (10, 12,1)\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "random_vector_for_generation = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "encoder = VAE_Encoder(latent_dim)\n",
        "decoder = VAE_Decoder(output_shape)\n",
        "model = Lagrange_Constrained_VAE(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc9iUq128M4U"
      },
      "outputs": [],
      "source": [
        "model2 = GMM_VAE(latent_dim, 3, encoder, decoder)\n",
        "model2.compile(optimizer = keras.optimizers.Adam(learning_rate = .0001))\n",
        "model2.fit(train_data, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSUD2wiq8Qfx"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate = .0001))\n",
        "history = model.fit(train_data, epochs = 10, callbacks = [beta_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7GHXQq37rn0"
      },
      "outputs": [],
      "source": [
        "for batch in train_data.take(1):  # Take only the first batch\n",
        "    x1 = batch\n",
        "x1[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPQkkul27z7c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file=\"vae_architecture.png\", show_shapes=True, show_layer_names=True, expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VajI-B8J75yH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x1[26].numpy().reshape(10, 12), cmap='gray')  # Reshape if needed\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(recons[26].numpy().reshape(10, 12), cmap = 'gray')  # Reshape if needed\n",
        "plt.title(\"Reconstructed\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF4YuRlB77Pk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
