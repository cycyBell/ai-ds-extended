{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1aMnlV44sNJt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulations\n",
        "\n",
        "This study will focus on 6 countries of the  Community of Sahel–Saharan States: *'Benin', 'Central African Rebublic', 'Chad', 'Niger', 'Soudan', 'Togo'*"
      ],
      "metadata": {
        "id": "qx9iqHve67ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crop_nutrient_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/cropland%20nutrient%20balance.csv\")\n",
        "crop_emission_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/crops%20emissions.csv\")\n",
        "natural_disaster_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/emdat_africa_1992-2023_natural.csv\")\n",
        "temperature_change = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/temperature%20change%20statistics.csv\")\n",
        "precipitation_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/gpcc-precipitation-1992-2022.csv\")\n",
        "pesticides_use_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/pesticides%20use.csv\")\n",
        "agriculture_emissions_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/Emissions%20from%20Energy%20use%20in%20agriculture.csv\")\n",
        "land_cover_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/Land%20Cover.csv\")\n",
        "prices_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/Prices_E_Africa.csv\")\n",
        "production_data = pd.read_csv(\"https://github.com/cycyBell/ai-ds-extended/raw/main/data_hub/Production_Crops_Livestock_E_Africa.csv\")"
      ],
      "metadata": {
        "id": "hh4-gmYAsXUt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_countries = ['Benin', 'Central African Rebublic', 'Chad', 'Niger', 'Soudan', 'Togo']\n"
      ],
      "metadata": {
        "id": "WYy6s-WV0xmZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_nutrient = crop_nutrient_data[crop_nutrient_data['Area'].isin(target_countries)]\n",
        "crop_emission = crop_emission_data[crop_emission_data['Area'].isin(target_countries)]\n",
        "temperature_change = temperature_change[temperature_change['Area'].isin(target_countries)]\n",
        "pesticides_use = pesticides_use_data[pesticides_use_data['Area'].isin(target_countries)]\n",
        "agriculture_emissions = agriculture_emissions_data[agriculture_emissions_data['Area'].isin(target_countries)]\n",
        "land_cover = land_cover_data[land_cover_data['Area'].isin(target_countries)]\n",
        "prices = prices_data[prices_data['Area'].isin(target_countries)]\n",
        "production = production_data[production_data['Area'].isin(target_countries)]"
      ],
      "metadata": {
        "id": "lnREqIuk6cxr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_nutrient.info()"
      ],
      "metadata": {
        "id": "FLIuaCvPueBg",
        "outputId": "47ceffb4-11e3-4395-965d-c96a62eb3443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 7936 entries, 3968 to 91531\n",
            "Data columns (total 15 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Domain Code       7936 non-null   object \n",
            " 1   Domain            7936 non-null   object \n",
            " 2   Area Code (M49)   7936 non-null   int64  \n",
            " 3   Area              7936 non-null   object \n",
            " 4   Element Code      7936 non-null   int64  \n",
            " 5   Element           7936 non-null   object \n",
            " 6   Item Code         7936 non-null   int64  \n",
            " 7   Item              7936 non-null   object \n",
            " 8   Year Code         7936 non-null   int64  \n",
            " 9   Year              7936 non-null   int64  \n",
            " 10  Unit              7936 non-null   object \n",
            " 11  Value             7936 non-null   float64\n",
            " 12  Flag              7936 non-null   object \n",
            " 13  Flag Description  7936 non-null   object \n",
            " 14  Note              0 non-null      float64\n",
            "dtypes: float64(2), int64(5), object(8)\n",
            "memory usage: 992.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model"
      ],
      "metadata": {
        "id": "--TGztWD7Cmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E09Bz2d13bex"
      },
      "outputs": [],
      "source": [
        "def xavier_normal_init(shape):\n",
        "  input_dim, output_dim = shape\n",
        "  std = tf.sqrt(2.)/tf.sqrt(tf.cast(input_dim + output_dim, dtype = tf.float32))\n",
        "  weight_values = tf.random.normal(shape, stddev=std)\n",
        "  return weight_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsmcpnot49Wl"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(tf.Module):\n",
        "  def __init__(self, output_dim, weight_init_fn = xavier_normal_init, activation = tf.identity):\n",
        "    self.output_dim = output_dim\n",
        "    self.weight_init_fn = weight_init_fn\n",
        "    self.activation = activation\n",
        "    self.built = False\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if not self.built:\n",
        "      self.input_dim = x.shape[-1]\n",
        "      self.weight = tf.Variable(self.weight_init_fn((self.input_dim, self.output_dim)), name = \"weight\")\n",
        "      self.bias = tf.Variable(tf.zeros((self.output_dim, )), name = \"bias\")\n",
        "      self.built = True\n",
        "    z = tf.add(tf.matmul(x, self.weight), self.bias)\n",
        "    return self.activation(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHl0VO9q5hMW"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n",
        "  def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epsilon = epsilon\n",
        "    self.t = 1.\n",
        "    self.v_dvar, self.s_dvar = [], []\n",
        "    self.built = False\n",
        "\n",
        "  def apply_gradients(self, grads, vars):\n",
        "    if not self.built:\n",
        "      for var in vars:\n",
        "        s = tf.Variable(tf.zeros(var.shape))\n",
        "        v = tf.Variable(tf.zeros(var.shape))\n",
        "        self.v_dvar.append(v)\n",
        "        self.s_dvar.append(s)\n",
        "      self.built = True\n",
        "    for i, (grad, var) in enumerate(zip(grads, vars)):\n",
        "      self.v_dvar[i].assign(self.beta_1 * self.v_dvar[i] + (1 - self.beta_1) * grad)\n",
        "      self.s_dvar[i].assign(self.beta_2 * self.s_dvar[i] + (1 - self.beta_2) * tf.square(grad))\n",
        "      v_corrected = self.v_dvar[i] / (1 - tf.pow(self.beta_1, self.t))\n",
        "      s_corrected = self.s_dvar[i] / (1 - tf.pow(self.beta_2, self.t))\n",
        "      var.assign_sub(self.learning_rate * v_corrected / (tf.sqrt(s_corrected) + self.epsilon))\n",
        "    self.t += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4cSc7cirqwm"
      },
      "outputs": [],
      "source": [
        "class BetaScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, beta, initial_beta=1.0, min_beta=0.1, decay_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.initial_beta = initial_beta\n",
        "        self.min_beta = min_beta\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        new_beta = max(self.min_beta, self.initial_beta - self.decay_rate * epoch)   #min(self.max_beta, (epoch + 1) / self.anneal_epochs * self.max_beta)\n",
        "        self.beta.assign(new_beta)\n",
        "        print(f\"Epoch {epoch+1}: Beta = {self.beta.numpy():.4f}\")\n",
        "\n",
        "# Initialize Beta\n",
        "beta = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "beta_scheduler = BetaScheduler(beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YapXM6o4859"
      },
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE_Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(Lagrange_Constrained_VAE_Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")\n",
        "    self.conv2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")  # 1 Conv Layer\n",
        "    self.pool = layers.MaxPooling2D((2, 2))\n",
        "    self.flatten = layers.GlobalMaxPooling2D()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.loc_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "    self.logvar_layer = layers.Dense(latent_dim)  # Log variance layer\n",
        "    self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "\n",
        "  def sample_latent(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch_size = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = keras.random.normal(shape=(batch_size, dim), seed = self.seed_generator)\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    mu = self.loc_layer(x)  # Mean of latent distribution\n",
        "    logvar = self.logvar_layer(x)  # Log variance\n",
        "    z = self.sample_latent([mu, logvar])\n",
        "\n",
        "    return mu, logvar, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnpAWfdbrqwp"
      },
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE_Decoder(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(Lagrange_Constrained_VAE_Decoder, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(64 * 5 * 6, activation=\"relu\")  # Increased spatial size\n",
        "        self.reshape = layers.Reshape((5, 6, 64))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", padding=\"same\")\n",
        "        self.convT3 = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        x = self.convT3(x)  # Final output (10,12,1)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izoAki5prqws"
      },
      "outputs": [],
      "source": [
        "class Lagrange_Constrained_VAE(keras.Model):\n",
        "    def __init__(self,encoder, decoder, lambda_lagrange_init = 1.0, **kwargs):\n",
        "        super(Lagrange_Constrained_VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.lambda_lagrange = tf.Variable(lambda_lagrange_init, trainable = True, dtype = tf.float32)\n",
        "        self.total_loss = keras.metrics.Mean(name = 'total_loss')\n",
        "        self.reconstruction_loss = keras.metrics.Mean(name = 'reconstruction_loss')\n",
        "        self.kl_loss = keras.metrics.Mean(name = 'kl_loss')\n",
        "        self.constraint_loss = keras.metrics.Mean(name = 'constraints_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "      return [\n",
        "              self.total_loss,\n",
        "              self.reconstruction_loss,\n",
        "              self.kl_loss,\n",
        "              self.constraint_loss\n",
        "      ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              keras.losses.binary_crossentropy(data, reconstruction), axis=(1)\n",
        "          )\n",
        "      )\n",
        "\n",
        "\n",
        "        tf.keras.losses.MeanSquaredError(reduction = \"sum_over_batch_size\")(data, reconstruction)\n",
        "\n",
        "\n",
        "\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        constraint_loss = tf.reduce_mean(self.lambda_lagrange*tf.square(tf.reduce_sum(z_log_var, axis = 1) - 1))\n",
        "        total_loss = reconstruction_loss + kl_loss + constraint_loss\n",
        "\n",
        "      grads = tape.gradient(total_loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "      self.total_loss.update_state(total_loss)\n",
        "      self.reconstruction_loss.update_state(reconstruction_loss)\n",
        "      self.kl_loss.update_state(kl_loss)\n",
        "      self.constraint_loss.update_state(constraint_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss.result(),\n",
        "          \"kl_loss\": self.kl_loss.result(),\n",
        "          \"constraint_loss\": self.constraint_loss.result()\n",
        "      }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW94LTBO48sI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VAE_Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(VAE_Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.dense2 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.loc_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "    self.logvar_layer = layers.Dense(latent_dim)  # Log variance layer\n",
        "    self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "\n",
        "  def sample_latent(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch_size = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = keras.random.normal(shape=(batch_size, dim), seed = self.seed_generator)\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    mu = self.loc_layer(x)  # Mean of latent distribution\n",
        "    logvar = self.logvar_layer(x)  # Log variance\n",
        "    z = self.sample_latent([mu, logvar])\n",
        "\n",
        "    return mu, logvar, z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ6LfCgD5y3b"
      },
      "outputs": [],
      "source": [
        "class VAE_Decoder(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(VAE_Decoder, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(128, activation=\"relu\")  # Increased spatial size\n",
        "        self.dense3 = layers.Dense(5*6*32, activation=\"relu\")\n",
        "        self.reshape = layers.Reshape((5, 6, 32))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XktY8rV6Igf"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.conv1 = layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.dense2 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.latent_layer = layers.Dense(latent_dim)  # Mean layer\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.flatten(x)  # Convert (10, 12) → (120)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    z = self.latent_layer(x)  # Mean of latent distribution\n",
        "\n",
        "\n",
        "    return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JM_BhZw6O1-"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__(self,output_shape=(10, 12, 1)):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "        # Expand latent vector to a feature map\n",
        "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = layers.Dense(128, activation=\"relu\")  # Increased spatial size\n",
        "        self.dense3 = layers.Dense(5*6*32, activation=\"relu\")\n",
        "        self.reshape = layers.Reshape((5, 6, 32))  # Larger reshaped feature map\n",
        "\n",
        "        # Upsampling to match (10,12)\n",
        "        self.convT1 = layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")\n",
        "        self.convT2 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid')  # Final output\n",
        "\n",
        "    def call(self, z):\n",
        "        x = self.dense1(z)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        x = self.reshape(x)  # Now (5,6,64)\n",
        "        x = self.convT1(x)  # Upsample to (10,12,32)\n",
        "        x = self.convT2(x)  # Keep spatial size (10,12,32)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkHC_jad6Tlk"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(keras.Model):\n",
        "    def __init__(self,encoder, decoder, **kwargs):\n",
        "        super(Autoencoder, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss = keras.metrics.Mean(name = 'total_loss')\n",
        "        self.reconstruction_loss = keras.metrics.Mean(name = 'reconstruction_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "      return [\n",
        "              self.total_loss,\n",
        "              self.reconstruction_loss,\n",
        "\n",
        "      ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "          tf.reduce_sum(\n",
        "              keras.losses.binary_crossentropy(data, reconstruction), axis=(1)\n",
        "          )\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "        total_loss = reconstruction_loss\n",
        "\n",
        "      grads = tape.gradient(total_loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "      self.total_loss.update_state(total_loss)\n",
        "      self.reconstruction_loss.update_state(reconstruction_loss)\n",
        "      return {\n",
        "          \"loss\": self.total_loss.result(),\n",
        "          \"reconstruction_loss\": self.reconstruction_loss.result(),\n",
        "\n",
        "      }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRRF3qlA6iy4"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "# set the dimensionality of the latent space to a plane for visualization later\n",
        "latent_dim = 2\n",
        "output_shape = (10, 12,1)\n",
        "encoder = Encoder(latent_dim)\n",
        "decoder = Decoder(output_shape)\n",
        "model1 = Autoencoder(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYwabjqz6kMX"
      },
      "outputs": [],
      "source": [
        "model1.compile(optimizer = keras.optimizers.Adam(learning_rate = .0001))\n",
        "history = model1.fit(train_data, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAn3c2jt8GVy"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "# set the dimensionality of the latent space to a plane for visualization later\n",
        "latent_dim = 2\n",
        "output_shape = (10, 12,1)\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "random_vector_for_generation = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "encoder = Lagrange_Constrained_VAE_Encoder(latent_dim)\n",
        "decoder = Lagrange_Constrained_VAE_Decoder(output_shape)\n",
        "model = Lagrange_Constrained_VAE(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSUD2wiq8Qfx"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate = .0001))\n",
        "history = model.fit(train_data, epochs = 10, callbacks = [beta_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7GHXQq37rn0"
      },
      "outputs": [],
      "source": [
        "for batch in train_data.take(1):  # Take only the first batch\n",
        "    x1 = batch\n",
        "x1[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPQkkul27z7c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file=\"vae_architecture.png\", show_shapes=True, show_layer_names=True, expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VajI-B8J75yH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x1[26].numpy().reshape(10, 12), cmap='gray')  # Reshape if needed\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(recons[26].numpy().reshape(10, 12), cmap = 'gray')  # Reshape if needed\n",
        "plt.title(\"Reconstructed\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF4YuRlB77Pk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}